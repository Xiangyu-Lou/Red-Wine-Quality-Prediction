{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from data_preprocess import load_training_data, load_test_data, normalize_features\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ((128, 64, 32), 0.1, 0.01, 500)\n",
      "Cross-validation mean accuracy: 58.71%\n",
      "Elapsed time: 0.83 seconds\n",
      "Number of iterations: 23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10150\\AppData\\Local\\Temp\\ipykernel_8960\\1156455548.py:60: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ((64, 32, 16), 0.1, 0.01, 500)\n",
      "Cross-validation mean accuracy: 58.35%\n",
      "Elapsed time: 0.47 seconds\n",
      "Number of iterations: 23\n",
      "\n",
      "Config: ((32, 16, 8), 0.1, 0.01, 500)\n",
      "Cross-validation mean accuracy: 56.92%\n",
      "Elapsed time: 0.23 seconds\n",
      "Number of iterations: 20\n",
      "\n",
      "Config: ((8, 16, 32), 0.1, 0.01, 500)\n",
      "Cross-validation mean accuracy: 59.07%\n",
      "Elapsed time: 0.21 seconds\n",
      "Number of iterations: 28\n",
      "\n",
      "Config: ((16, 32, 64), 0.1, 0.01, 500)\n",
      "Cross-validation mean accuracy: 57.28%\n",
      "Elapsed time: 0.45 seconds\n",
      "Number of iterations: 25\n",
      "\n",
      "Config: ((32, 64, 128), 0.1, 0.01, 500)\n",
      "Cross-validation mean accuracy: 58.00%\n",
      "Elapsed time: 0.68 seconds\n",
      "Number of iterations: 23\n",
      "\n",
      "Config: ((8, 16, 32, 64, 128), 0.1, 0.01, 500)\n",
      "Cross-validation mean accuracy: 58.27%\n",
      "Elapsed time: 0.95 seconds\n",
      "Number of iterations: 29\n",
      "\n",
      "Config: ((128, 64, 32, 16, 8), 0.1, 0.01, 500)\n",
      "Cross-validation mean accuracy: 58.90%\n",
      "Elapsed time: 1.01 seconds\n",
      "Number of iterations: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "X_train, y_train = load_training_data('../data/train.csv')\n",
    "\n",
    "# Normalize the features\n",
    "X_train = normalize_features(X_train)\n",
    "\n",
    "# Initialize variables\n",
    "hidden_layer_sizes = [(128,64,32), (64,32,16), (32,16,8), (8, 16, 32), (16, 32, 64), (32, 64, 128), (8, 16, 32, 64, 128), (128, 64, 32, 16, 8)]\n",
    "alpha_values = [0.1]\n",
    "learning_rate_init_values = [0.01] \n",
    "max_iter_values = [500]\n",
    "results = {}\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['hidden_layer_size', 'alpha', 'learning_rate_init', 'max_iter', 'accuracy', 'f1_score'])\n",
    "\n",
    "# Define 5-fold cross validation test harness\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    for alpha in alpha_values:\n",
    "        for learning_rate_init in learning_rate_init_values:\n",
    "            for max_iter in max_iter_values:\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Define the model with early stopping\n",
    "                model = MLPClassifier(hidden_layer_sizes=hidden_layer_size, activation='relu', solver='adam',\n",
    "                                      max_iter=max_iter, alpha=alpha, learning_rate_init=learning_rate_init,\n",
    "                                      early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "                # Perform cross-validation manually to get the number of iterations\n",
    "                cv_results = []\n",
    "                f1_results = []\n",
    "                for train_index, test_index in kfold.split(X_train):\n",
    "                    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "                    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "                    model.fit(X_train_fold, y_train_fold)\n",
    "                    score = model.score(X_test_fold, y_test_fold)\n",
    "                    cv_results.append(score)\n",
    "                    y_pred = model.predict(X_test_fold)\n",
    "                    f1 = f1_score(y_test_fold, y_pred, average='weighted')\n",
    "                    f1_results.append(f1)\n",
    "\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time - start_time\n",
    "                config = (hidden_layer_size, alpha, learning_rate_init, max_iter)\n",
    "                print(\"Config: {}\\nCross-validation mean accuracy: {:.2f}%\\nElapsed time: {:.2f} seconds\\nNumber of iterations: {}\\n\".format(\n",
    "                    config, np.mean(cv_results)*100, elapsed_time, model.n_iter_))\n",
    "\n",
    "                new_row = pd.DataFrame({\n",
    "                    'hidden_layer_size': [hidden_layer_size],\n",
    "                    'alpha': [alpha],\n",
    "                    'learning_rate_init': [learning_rate_init],\n",
    "                    'max_iter': [max_iter],\n",
    "                    'accuracy': [np.mean(cv_results)],\n",
    "                    'f1_score': [np.mean(f1_results)]\n",
    "                })\n",
    "\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's paramemters with the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: hidden_layer_size     (8, 16, 32)\n",
      "alpha                         0.1\n",
      "learning_rate_init           0.01\n",
      "max_iter                      500\n",
      "accuracy                 0.590739\n",
      "f1_score                 0.558756\n",
      "Name: 3, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/ann.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the best parameters\n",
    "best_params = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = MLPClassifier(hidden_layer_sizes=best_params['hidden_layer_size'], \n",
    "                            activation='relu', \n",
    "                            solver='adam', \n",
    "                            max_iter=best_params['max_iter'], \n",
    "                            alpha=best_params['alpha'], \n",
    "                            learning_rate_init=best_params['learning_rate_init'], \n",
    "                            early_stopping=True, \n",
    "                            n_iter_no_change=10)\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the final model\n",
    "joblib.dump(final_model, 'model/ann.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "# Load the final model\n",
    "final_model = joblib.load('model/ann.joblib')\n",
    "\n",
    "# Load the test data\n",
    "X_test, y_test = load_test_data('../data/test.csv', 'quality')\n",
    "\n",
    "# Normalize the test data\n",
    "X_test_normalized = normalize_features(X_test)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = final_model.predict(X_test_normalized)\n",
    "\n",
    "# Calculate the accuracy of the model on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
